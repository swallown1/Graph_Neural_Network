{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "\n",
    "from keras.layers import Dropout,Input\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,TensorBoard,ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from layer import GraphAtten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('cora')\n",
    "# A 邻接矩阵(2708, 2708)  X 是X的features  mutil-hot  (2708, 1433)   Y_train (2708, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = Y_train.shape[1]  # Number of classes\n",
    "F_ = 8                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2               # Factor for l2 regularization\n",
    "learning_rate = 5e-3          # Learning rate for Adam\n",
    "epochs = 10               # Number of training epochs\n",
    "es_patience = 100             # Patience fot early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing operations \n",
    "X = preprocess_features(X)  ## 进行行归一化\n",
    "A = A + np.eye(A.shape[0])  # Add self-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition (as per Section 3.3 of the paper)\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1 = Dropout(dropout_rate)(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_attention_1 = GraphAtten(F_,\n",
    "        \n",
    "                               attn_heads = n_attn_heads,\n",
    "                              attn_heads_reduction='concat',\n",
    "                              dropout_rate=dropout_rate,\n",
    "                              activation='elu',\n",
    "                              kernel_regularizer=l2(l2_reg),\n",
    "                              attn_kernel_regularizer=l2(l2_reg))([dropout1,A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAtten(n_classes,\n",
    "                              attn_heads = 1,\n",
    "                              attn_heads_reduction='average',\n",
    "                              dropout_rate=dropout_rate,\n",
    "                              activation='softmax',\n",
    "                              kernel_regularizer=l2(l2_reg),\n",
    "                              attn_kernel_regularizer=l2(l2_reg))([dropout2,A_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_atten_1 (GraphAtten)      (None, 64)           91904       dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 64)           0           graph_atten_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "graph_atten_2 (GraphAtten)      (None, 7)            469         dropout_18[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 92,373\n",
      "Trainable params: 92,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2708 samples, validate on 2708 samples\n",
      "Epoch 1/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.7865 - weighted_acc: 0.1000 - val_loss: 2.0245 - val_weighted_acc: 0.0720\n",
      "Epoch 2/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.5931 - weighted_acc: 0.1357 - val_loss: 1.9962 - val_weighted_acc: 0.0720\n",
      "Epoch 3/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.4461 - weighted_acc: 0.1500 - val_loss: 1.9776 - val_weighted_acc: 0.0720\n",
      "Epoch 4/10\n",
      "2708/2708 [==============================] - 12s 4ms/step - loss: 2.4598 - weighted_acc: 0.2357 - val_loss: 1.9655 - val_weighted_acc: 0.0720\n",
      "Epoch 5/10\n",
      "2708/2708 [==============================] - 12s 4ms/step - loss: 2.4623 - weighted_acc: 0.1357 - val_loss: 1.9670 - val_weighted_acc: 0.0720\n",
      "Epoch 6/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.4115 - weighted_acc: 0.1500 - val_loss: 1.9671 - val_weighted_acc: 0.0720\n",
      "Epoch 7/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.4456 - weighted_acc: 0.1500 - val_loss: 1.9648 - val_weighted_acc: 0.0720\n",
      "Epoch 8/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.4317 - weighted_acc: 0.1357 - val_loss: 1.9619 - val_weighted_acc: 0.0720\n",
      "Epoch 9/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.4611 - weighted_acc: 0.1500 - val_loss: 1.9597 - val_weighted_acc: 0.0720\n",
      "Epoch 10/10\n",
      "2708/2708 [==============================] - 10s 4ms/step - loss: 2.3651 - weighted_acc: 0.1286 - val_loss: 1.9589 - val_weighted_acc: 0.0720\n",
      "Done.\n",
      "Test loss: 2.0021016597747803\n",
      "Test accuracy: 0.09100000560283661\n"
     ]
    }
   ],
   "source": [
    "# build Model\n",
    "\n",
    "model = Model(inputs=[X_in,A_in],outputs=graph_attention_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             weighted_metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(log_dir='./logs/loss',  # log 目录\n",
    "                 histogram_freq=0,  # 按照何等频率（epoch）来计算直方图，0为不计算\n",
    "#                  batch_size=32,     # 用多大量的数据计算直方图\n",
    "                 write_graph=True,  # 是否存储网络结构图\n",
    "                 write_grads=True, # 是否可视化梯度直方图\n",
    "                 write_images=True,# 是否可视化参数\n",
    "                batch_size=N)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([X, A], Y_val, idx_val)\n",
    "model.fit([X, A],\n",
    "          Y_train,\n",
    "          sample_weight=idx_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback, tb_callback, mc_callback])\n",
    "\n",
    "# Load best model\n",
    "model.load_weights('logs/best_model.h5')\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = model.evaluate([X, A],\n",
    "                              Y_test,\n",
    "                              sample_weight=idx_test,\n",
    "                              batch_size=N,\n",
    "                              verbose=0)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
