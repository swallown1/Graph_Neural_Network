{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser as cp\n",
    "import re, os\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from DataModule import  DataModule\n",
    "from Evaluate import Evaluate\n",
    "from diffnet import diffnet\n",
    "from Logging import Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.getcwd(), 'data/yelp_diffnet.ini')\n",
    "model_name = 'diffnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserConf():\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        self.config_path = config_path\n",
    "        \n",
    "    def processValue(self,key,value):\n",
    "        print(key, value)\n",
    "        tmp = value.split(' ')\n",
    "        dtype = tmp[0]\n",
    "        value = tmp[1:]\n",
    "#         print(dtype, value)\n",
    "        \n",
    "        if value != None:\n",
    "            if dtype == 'string':\n",
    "                self.conf_dict[key] = vars(self)[key] = value[0]\n",
    "            elif dtype == 'int':\n",
    "                self.conf_dict[key] = vars(self)[key] = int(value[0])\n",
    "            elif dtype == 'float':\n",
    "                self.conf_dict[key] = vars(self)[key] = float(value[0])\n",
    "            elif dtype == 'list':\n",
    "                self.conf_dict[key] = vars(self)[key] = [i for i in value]\n",
    "            elif dtype == 'int_list':\n",
    "                self.conf_dict[key] = vars(self)[key] = [int(i) for i in value]\n",
    "            elif dtype == 'float_list':\n",
    "                self.conf_dict[key] = vars(self)[key] = [float(i) for i in value]\n",
    "        else:\n",
    "            print('%s value is None' % key)\n",
    "    \n",
    "    def parserConf(self):\n",
    "        conf = cp.ConfigParser()\n",
    "        conf.read(self.config_path)\n",
    "        self.conf = conf\n",
    "        \n",
    "        self.conf_dict = {}\n",
    "        for section in conf.sections():\n",
    "            for (key, value) in conf.items(section):\n",
    "                self.processValue(key, value)\n",
    "        \n",
    "        self.data_dir = os.path.join(os.getcwd(),'data')\n",
    "        self.links_filename = os.path.join(os.getcwd(),'data/yelp.links')\n",
    "        self.user_review_vector_matrix=os.path.join(os.getcwd(), 'data/user_vector.npy')\n",
    "        self.item_review_vector_matrix = os.path.join(os.getcwd(), 'data/item_vector.npy')\n",
    "#         self.pre_model = os.path.join(os.getcwd(), 'pretrain/%s/%s' % (self.data_name, self.pre_model))\n",
    "\n",
    "class DataUtil():\n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "        #print('DataUtil, Line12, test- conf data_dir:%s' % self.conf.data_dir)\n",
    "\n",
    "    def initializeRankingHandle(self):\n",
    "        #t0 = time()\n",
    "        self.createTrainHandle()\n",
    "        self.createEvaluateHandle()\n",
    "        #t1 = time()\n",
    "        #print('Prepare data cost:%.4fs' % (t1 - t0))\n",
    "    \n",
    "    def createTrainHandle(self):\n",
    "        data_dir = self.conf.data_dir\n",
    "        # train  data\n",
    "        train_filename = \"%s/%s.train.rating\" % (data_dir, self.conf.data_name)\n",
    "        val_filename = \"%s/%s.val.rating\" % (data_dir, self.conf.data_name)\n",
    "        test_filename = \"%s/%s.test.rating\" % (data_dir, self.conf.data_name)\n",
    "\n",
    "        self.train = DataModule(self.conf,train_filename)\n",
    "        self.val = DataModule(self.conf,val_filename)\n",
    "        self.test = DataModule(self.conf,test_filename)\n",
    "        \n",
    "\n",
    " \n",
    "    def createEvaluateHandle(self):\n",
    "        data_dir = self.conf.data_dir\n",
    "        # eval data\n",
    "        val_filename = \"%s/%s.val.rating\" % (data_dir, self.conf.data_name)\n",
    "        test_filename = \"%s/%s.test.rating\" % (data_dir, self.conf.data_name)\n",
    "\n",
    "        self.val_eva = DataModule(self.conf, val_filename)\n",
    "        self.test_eva = DataModule(self.conf, test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users int 17237\n",
      "num_items int 38342\n",
      "gpu_device int 1\n",
      "data_name string yelp\n",
      "model_name string diffnet\n",
      "dimension int 32\n",
      "learning_rate float 0.001\n",
      "epochs int 10\n",
      "num_negatives int 8\n",
      "num_evaluate int 1000\n",
      "num_procs int 16\n",
      "topk int 10\n",
      "evaluate_batch_size int 128\n",
      "training_batch_size int 128\n",
      "epoch_notice int 300\n",
      "pretrain_flag int 1\n",
      "pre_model string diffnet_hr_0.3437_ndcg_0.2092_epoch_98.ckpt\n"
     ]
    }
   ],
   "source": [
    "conf = ParserConf(config_path)\n",
    "conf.parserConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'yelp',\n",
       " 'dimension': 32,\n",
       " 'epoch_notice': 300,\n",
       " 'epochs': 10,\n",
       " 'evaluate_batch_size': 128,\n",
       " 'gpu_device': 1,\n",
       " 'learning_rate': 0.001,\n",
       " 'model_name': 'diffnet',\n",
       " 'num_evaluate': 1000,\n",
       " 'num_items': 38342,\n",
       " 'num_negatives': 8,\n",
       " 'num_procs': 16,\n",
       " 'num_users': 17237,\n",
       " 'pre_model': 'diffnet_hr_0.3437_ndcg_0.2092_epoch_98.ckpt',\n",
       " 'pretrain_flag': 1,\n",
       " 'topk': 10,\n",
       " 'training_batch_size': 128}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.conf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataUtil(conf)\n",
    "model = eval(model_name)\n",
    "model = model(conf)\n",
    "evaluate = Evaluate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(os.getcwd(), 'log')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "# define log name \n",
    "log_path = os.path.join(os.getcwd(), 'log/%s_%s.log' % (conf.data_name, conf.model_name))\n",
    "\n",
    "data.initializeRankingHandle()\n",
    "\n",
    "d_train, d_val, d_test, d_test_eva = data.train, data.val, data.test, data.test_eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System start to load data...\n",
      "Data has been loaded successfully, cost:24.0019s\n"
     ]
    }
   ],
   "source": [
    "print('System start to load data...')\n",
    "t0 = time()\n",
    "d_train.initializeRankingTrain()\n",
    "d_val.initializeRankingVT()\n",
    "d_test.initializeRankingVT()\n",
    "d_test_eva.initalizeRankingEva()\n",
    "t1 = time()\n",
    "print('Data has been loaded successfully, cost:%.4fs' % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model necessary data.\n",
    "data_dict = d_train.prepareModelSupplement(model)\n",
    "model.inputSupply(data_dict)\n",
    "model.startConstructGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard tensorflow running environment initialize\n",
    "tf_conf = tf.ConfigProto()\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# tf_conf.gpu_options.allow_growth = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following will output the evaluation of the model:\n",
      "19445676.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "G:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf_conf) as sess:\n",
    "    sess.run(model.init)\n",
    "#     if conf.pretrain_flag == 1:\n",
    "#         model.saver.restore(sess, conf.pre_model)\n",
    "\n",
    "    # set debug_flag=0, doesn't print any results\n",
    "    log = Logging(log_path)\n",
    "    log.record('Following will output the evaluation of the model:')\n",
    "    # Start Training !!!\n",
    "    for epoch in range(1, conf.epochs+1):\n",
    "        # optimize model with training data and compute train loss\n",
    "        tmp_train_loss = []\n",
    "        t0 = time()\n",
    "\n",
    "        #tmp_total_list = []\n",
    "        while d_train.terminal_flag:\n",
    "            d_train.getTrainRankingBatch()\n",
    "            d_train.linkedMap()\n",
    "\n",
    "            train_feed_dict = {}\n",
    "            for (key, value) in model.map_dict['train'].items():\n",
    "                train_feed_dict[key] = d_train.data_dict[value]\n",
    "                \n",
    "            [sub_train_loss, _] = sess.run([model.map_dict['out']['train'], model.opt], feed_dict=train_feed_dict)\n",
    "            tmp_train_loss.append(sub_train_loss)\n",
    "        train_loss = np.mean(tmp_train_loss)\n",
    "        t1 = time()\n",
    "\n",
    "        # compute val loss and test loss\n",
    "        d_val.getVTRankingOneBatch()\n",
    "        d_val.linkedMap()\n",
    "        val_feed_dict = {}\n",
    "        for (key, value) in model.map_dict['val'].items():\n",
    "            val_feed_dict[key] = d_val.data_dict[value]\n",
    "        val_loss = sess.run(model.map_dict['out']['val'], feed_dict=val_feed_dict)\n",
    "\n",
    "        d_test.getVTRankingOneBatch()\n",
    "        d_test.linkedMap()\n",
    "        test_feed_dict = {}\n",
    "        for (key, value) in model.map_dict['test'].items():\n",
    "            test_feed_dict[key] = d_test.data_dict[value]\n",
    "        test_loss = sess.run(model.map_dict['out']['test'], feed_dict=test_feed_dict)\n",
    "        t2 = time()\n",
    "\n",
    "        # start evaluate model performance, hr and ndcg\n",
    "        def getPositivePredictions():\n",
    "            d_test_eva.getEvaPositiveBatch()\n",
    "            d_test_eva.linkedRankingEvaMap()\n",
    "            eva_feed_dict = {}\n",
    "            for (key, value) in model.map_dict['eva'].items():\n",
    "                eva_feed_dict[key] = d_test_eva.data_dict[value]\n",
    "            positive_predictions = sess.run(\n",
    "                model.map_dict['out']['eva'],\n",
    "                feed_dict=eva_feed_dict\n",
    "            )\n",
    "            return positive_predictions\n",
    "\n",
    "        def getNegativePredictions():\n",
    "            negative_predictions = {}\n",
    "            terminal_flag = 1\n",
    "            while terminal_flag:\n",
    "                batch_user_list, terminal_flag = d_test_eva.getEvaRankingBatch()\n",
    "                d_test_eva.linkedRankingEvaMap()\n",
    "                eva_feed_dict = {}\n",
    "                for (key, value) in model.map_dict['eva'].items():\n",
    "                    eva_feed_dict[key] = d_test_eva.data_dict[value]\n",
    "                index = 0\n",
    "                tmp_negative_predictions = np.reshape(\n",
    "                    sess.run(\n",
    "                        model.map_dict['out']['eva'],\n",
    "                        feed_dict=eva_feed_dict\n",
    "                    ),\n",
    "                    [-1, conf.num_evaluate])\n",
    "                for u in batch_user_list:\n",
    "                    negative_predictions[u] = tmp_negative_predictions[index]\n",
    "                    index = index + 1\n",
    "            return negative_predictions\n",
    "\n",
    "        tt2 = time()\n",
    "\n",
    "        index_dict = d_test_eva.eva_index_dict\n",
    "        positive_predictions = getPositivePredictions()\n",
    "        negative_predictions = getNegativePredictions()\n",
    "\n",
    "        d_test_eva.index = 0 # !!!important, prepare for new batch\n",
    "        hr, ndcg = evaluate.evaluateRankingPerformance(\\\n",
    "            index_dict, positive_predictions, negative_predictions, conf.topk, conf.num_procs)\n",
    "        tt3 = time()\n",
    "                \n",
    "        # print log to console and log_file\n",
    "        log.record('Epoch:%d, compute loss cost:%.4fs, train loss:%.4f, val loss:%.4f, test loss:%.4f' % \\\n",
    "            (epoch, (t2-t0), train_loss, val_loss, test_loss))\n",
    "        log.record('Evaluate cost:%.4fs, hr:%.4f, ndcg:%.4f' % ((tt3-tt2), hr, ndcg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
