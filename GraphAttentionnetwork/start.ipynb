{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_attention_layer import GraphAttention\n",
    "from  untils import load_data,preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X的shape: (2708, 1433)\n",
      "A的shape: (2708, 2708)\n",
      "Y_train的shape: (2708, 7)\n"
     ]
    }
   ],
   "source": [
    "print('X的shape:',X.shape)\n",
    "print('A的shape:',A.shape)\n",
    "print('Y_train的shape:',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = Y_train.shape[1]  #标签的维度\n",
    "F_ = 8                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads  即K值\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2\n",
    "learning_rate = 5e-3\n",
    "epochs = 10\n",
    "es_patience = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对行进行规范化\n",
    "X= preprocess_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A +np.eye(A.shape[0])  #加上自连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_1 (GraphAttenti (None, 64)           91904       dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 64)           0           graph_attention_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_2 (GraphAttenti (None, 7)            469         dropout_18[0][0]                 \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 92,373\n",
      "Trainable params: 92,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 定义模型输入\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "\n",
    "#第一次注意力层\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                  attn_heads=n_attn_heads,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  activation='elu',\n",
    "                                  kernel_regularizer=l2(l2_reg),\n",
    "                                  attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "\n",
    "\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "\n",
    "#第二次注意力层\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                  attn_heads=1,\n",
    "                                  attn_heads_reduction='average',\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_regularizer=l2(l2_reg),\n",
    "                                  attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer = optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             weighted_metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "# tb_callback = TensorBoard(batch_size=N)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2708 samples, validate on 2708 samples\n",
      "Epoch 1/10\n",
      "2708/2708 [==============================] - 2s 749us/step - loss: 0.1139 - acc: 0.1643 - val_loss: 0.3694 - val_acc: 0.3260\n",
      "Epoch 2/10\n",
      "2708/2708 [==============================] - 2s 755us/step - loss: 0.1126 - acc: 0.2071 - val_loss: 0.3683 - val_acc: 0.3160\n",
      "Epoch 3/10\n",
      "2708/2708 [==============================] - 2s 746us/step - loss: 0.1118 - acc: 0.1786 - val_loss: 0.3673 - val_acc: 0.3160\n",
      "Epoch 4/10\n",
      "2708/2708 [==============================] - 2s 722us/step - loss: 0.1108 - acc: 0.1643 - val_loss: 0.3668 - val_acc: 0.3160\n",
      "Epoch 5/10\n",
      "2708/2708 [==============================] - 2s 744us/step - loss: 0.1101 - acc: 0.2071 - val_loss: 0.3664 - val_acc: 0.3180\n",
      "Epoch 6/10\n",
      "2708/2708 [==============================] - 2s 734us/step - loss: 0.1099 - acc: 0.2000 - val_loss: 0.3663 - val_acc: 0.3220\n",
      "Epoch 7/10\n",
      "2708/2708 [==============================] - 2s 790us/step - loss: 0.1096 - acc: 0.2143 - val_loss: 0.3663 - val_acc: 0.3620\n",
      "Epoch 8/10\n",
      "2708/2708 [==============================] - 2s 754us/step - loss: 0.1094 - acc: 0.2286 - val_loss: 0.3663 - val_acc: 0.4540\n",
      "Epoch 9/10\n",
      "2708/2708 [==============================] - 2s 811us/step - loss: 0.1094 - acc: 0.2357 - val_loss: 0.3663 - val_acc: 0.5580\n",
      "Epoch 10/10\n",
      "2708/2708 [==============================] - 2s 753us/step - loss: 0.1094 - acc: 0.1643 - val_loss: 0.3664 - val_acc: 0.3980\n",
      "Done.\n",
      "Test loss: 0.7233245372772217\n",
      "Test accuracy: 0.4300000071525574\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "validation_data = ([X,A],Y_val,idx_val)\n",
    "model.fit([X,A],\n",
    "         Y_train,\n",
    "        sample_weight=idx_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,\n",
    "          callbacks=[es_callback,mc_callback])\n",
    "# callbacks=[es_callback,tb_callback,mc_callback]\n",
    "# Test model\n",
    "eval_results = model.evaluate([X,A],\n",
    "                             Y_test,\n",
    "                             sample_weight=idx_test,\n",
    "                             batch_size=N,\n",
    "                             verbose=0)\n",
    "\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
